import pandas as pd
from openpyxl import load_workbook

import os
import sys
import json
from IPython.display import display

def get_exe_base_dir():
    if getattr(sys, 'frozen', False):
        return os.path.dirname(sys.executable)
    else:
        return os.path.dirname(os.path.abspath(__file__))
    
def get_base_dir():
    """
    âœ… è·å–ç¨‹åºçœŸå®è¿è¡Œç›®å½•ï¼š
    - PyInstaller æ‰“åŒ…å â†’ dist/single_quater(.exe/.app)
    - æºç è¿è¡Œ â†’ utils.py æ‰€åœ¨ç›®å½•
    """
    if getattr(sys, "frozen", False):
        return os.path.dirname(sys.executable)
    return os.path.dirname(os.path.abspath(__file__))

def run_all_pipelines_and_save_intermediate(
    quarter_folder: str,     # ä¾‹å¦‚ "Q4"
    year: int,
    quarter: str,            # "Q1" / "Q2" / "Q3" / "Q4"
    save_dir: str
):
    """
    âœ… æœ€ç»ˆç»Ÿä¸€è¾“å‡ºè§„èŒƒç‰ˆï¼š
    - è‡ªåŠ¨åŒ¹é… IND / NDA / FDA / NMPA å››ä¸ªæ–‡ä»¶
    - åˆ†åˆ«è·‘å››å¥—æµæ°´çº¿
    - âœ… æ‰€æœ‰ output_file å’Œä¸­é—´é‡ï¼Œç»Ÿä¸€ä¿å­˜åˆ°ï¼š
        Q4_intermediate / Q3_intermediate è¿™ç§æ–‡ä»¶å¤¹ä¸­
    - âœ… é˜²æ­¢ä»»ä½•æ–‡ä»¶äº’ç›¸è¦†ç›–
    """

    import os

    # ===== âœ… 0ï¸âƒ£ ç»Ÿä¸€ä¸­é—´ç›®å½•å‘½å =====
    intermediate_dir = os.path.join(save_dir,f"{quarter}_intermediate")

    os.makedirs(intermediate_dir, exist_ok=True)

    print("\n==============================")
    print("ğŸš€ å¼€å§‹è‡ªåŠ¨è¿è¡Œå››å¤§ç›‘ç®¡æµæ°´çº¿")
    # âœ… å…³é”®ï¼šå¼ºåˆ¶è¾“å‡ºåˆ° dist ç›®å½•
    # base_dir = os.path.dirname(os.path.abspath(__file__))   # dist ç›®å½•
    # intermediate_dir = os.path.join(base_dir, f"{quarter}_intermediate")
    # os.makedirs(intermediate_dir, exist_ok=True)

    print(f"ğŸ“ ç»Ÿä¸€è¾“å‡ºç›®å½•ï¼š{intermediate_dir}")
    print("==============================\n")

    # ===== âœ… 1ï¸âƒ£ è‡ªåŠ¨åŒ¹é…æ–‡ä»¶ =====
    file_paths = match_regulatory_files(quarter_folder)

    ind_file  = file_paths.get("IND")
    nda_file  = file_paths.get("NDA")
    fda_file  = file_paths.get("FDA")
    nmpa_file = file_paths.get("NMPA")

    results = {}
    stats_dict = {}

    # =============================
    # âœ… 2ï¸âƒ£ IND
    # =============================
    if ind_file:
        ind_out = os.path.join(intermediate_dir, f"{quarter}_IND_ç»“æœ.xlsx")

        res_ind = run_ind_nda_pipeline(
            input_file=ind_file,
            output_file=ind_out,
            source="IND"
        )

        # save_intermediate_df(df_ind, intermediate_dir, f"{quarter}_IND")
        df_ind = res_ind["df"]
        
        stats_dict["China IND"] = {
            "ã€ç²—åˆ†ç±»ç»Ÿè®¡ã€‘": res_ind["stat_coarse"],
            "ã€ç–¾ç—…é¢†åŸŸç»Ÿè®¡ã€‘": res_ind["stat_disease"],
            "ã€é¶ç‚¹ç»Ÿè®¡ã€‘": res_ind["stat_target"]
        }
        results["IND"] = df_ind
    else:
        print("âš ï¸ æœªæ‰¾åˆ° IND æ–‡ä»¶ï¼Œå·²è·³è¿‡")

    # =============================
    # âœ… 3ï¸âƒ£ NDA
    # =============================
    if nda_file:
        nda_out = os.path.join(intermediate_dir, f"{quarter}_NDA_ç»“æœ.xlsx")

        res_nda = run_ind_nda_pipeline(
            input_file=nda_file,
            output_file=nda_out,
            source="NDA"
        )

        # save_intermediate_df(df_nda, intermediate_dir, f"{quarter}_NDA")
        df_nda = res_nda["df"]

        stats_dict["China NDA"] = {
            "ã€ç²—åˆ†ç±»ç»Ÿè®¡ã€‘": res_nda["stat_coarse"],
            "ã€ç–¾ç—…é¢†åŸŸç»Ÿè®¡ã€‘": res_nda["stat_disease"],
            "ã€é¶ç‚¹ç»Ÿè®¡ã€‘": res_nda["stat_target"]
        }
        results["NDA"] = df_nda
    else:
        print("âš ï¸ æœªæ‰¾åˆ° NDA æ–‡ä»¶ï¼Œå·²è·³è¿‡")

    # =============================
    # âœ… 4ï¸âƒ£ FDA
    # =============================
    if fda_file:
        fda_out = os.path.join(intermediate_dir, f"{quarter}_FDA_ç»“æœ.xlsx")

        res_fda = run_fda_pipeline(
            input_file=fda_file,
            output_file=fda_out
        )

        # save_intermediate_df(df_fda, intermediate_dir, f"{quarter}_FDA")
        df_fda = res_fda["df"]

        stats_dict["FDA approved drugs"] = {
            "ã€ç²—åˆ†ç±»ç»Ÿè®¡ã€‘": res_fda["stat_coarse"],
            "ã€é¶ç‚¹ç»Ÿè®¡ã€‘": res_fda["stat_target"]
        }

        results["FDA"] = df_fda
    else:
        print("âš ï¸ æœªæ‰¾åˆ° FDA æ–‡ä»¶ï¼Œå·²è·³è¿‡")

    # =============================
    # âœ… 5ï¸âƒ£ NMPA
    # =============================
    if nmpa_file:
        nmpa_out = os.path.join(intermediate_dir, f"{quarter}_NMPA_ç»“æœ.xlsx")

        res_nmpa = run_nmpa_quarter_pipeline(
            input_file=nmpa_file,
            output_file=nmpa_out,
            year=year,
            quarter=quarter
        )

        # save_intermediate_df(df_nmpa, intermediate_dir, f"{quarter}_NMPA")
        df_nmpa = res_nmpa["df"]

        stats_dict["NMPA approved drugs"] = {
            "ã€ç²—åˆ†ç±»ç»Ÿè®¡ã€‘": res_nmpa["stat_coarse"],
            "ã€ç–¾ç—…é¢†åŸŸç»Ÿè®¡ã€‘": res_nmpa["stat_disease"],
            "ã€é¶ç‚¹ç»Ÿè®¡ã€‘": res_nmpa["stat_target"]
        }
        results["NMPA"] = df_nmpa
    else:
        print("âš ï¸ æœªæ‰¾åˆ° NMPA æ–‡ä»¶ï¼Œå·²è·³è¿‡")

    print("\n==============================")
    print("âœ… å››å¤§ç›‘ç®¡æµæ°´çº¿å…¨éƒ¨æ‰§è¡Œå®Œæˆ")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœ & ä¸­é—´é‡ç»Ÿä¸€ä¿å­˜åœ¨ï¼š{intermediate_dir}")
    print("==============================\n")

    return results,stats_dict

def get_exe_base_dir():
    """
    âœ… è·å–ç¨‹åºçœŸå®è¿è¡Œç›®å½•ï¼š
    - PyInstaller æ‰“åŒ…å â†’ dist/
    - æºç è¿è¡Œ â†’ utils.py æ‰€åœ¨ç›®å½•
    """
    if getattr(sys, 'frozen', False):
        return os.path.dirname(sys.executable)
    else:
        return os.path.dirname(os.path.abspath(__file__))


def match_regulatory_files(quarter_folder: str):
    """
    âœ… æ°¸è¿œä»ã€ç¨‹åºæ‰€åœ¨ç›®å½•ã€‘ä¸‹é¢æ‰¾ Q4 / Q1 / Q2 ç›®å½•
    """
    base_dir = get_exe_base_dir()
    quarter_dir = os.path.join(base_dir, quarter_folder)

    print(f"ğŸ“ å®é™…æœç´¢ç›®å½•ï¼š{quarter_dir}")

    if not os.path.exists(quarter_dir):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°å­£åº¦æ–‡ä»¶å¤¹ï¼š{quarter_dir}")

    files = os.listdir(quarter_dir)

    result = {"IND": None, "NDA": None, "FDA": None, "NMPA": None}

    for f in files:
        f_upper = f.upper()
        full_path = os.path.join(quarter_dir, f)

        if "IND" in f_upper:
            result["IND"] = full_path
        elif "NDA" in f_upper:
            result["NDA"] = full_path
        elif "FDA" in f_upper:
            result["FDA"] = full_path
        elif "NMPA" in f_upper:
            result["NMPA"] = full_path

    print("âœ… åŒ¹é…åˆ°çš„ç›‘ç®¡æ–‡ä»¶ï¼š")
    for k, v in result.items():
        print(f"   {k}: {v}")

    return result

def step1_dedup_only_keep_latest_NDA_IND(
    input_path: str,
    sheet_name: str = "æ•°æ®è¯¦æƒ…",
    date_col: str = "CDEæ‰¿åŠæ—¥æœŸ",
):
    df = pd.read_excel(input_path, sheet_name=sheet_name)

    print("âœ… åŸå§‹æ•°æ®è¡Œæ•°ï¼š", len(df))
    # display(df.head())

    # ===== å»é‡é”®ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨ =====
    dedup_cols = ["é€šç”¨å", "å‰‚å‹", "æŒè¯å•†"]
    missing = [c for c in dedup_cols if c not in df.columns]

    if missing:
        print(f"âš ï¸ æœªæ‰¾åˆ°å»é‡å…³é”®åˆ—ï¼š{missing}ï¼Œå°†è·³è¿‡å»é‡ï¼Œç›´æ¥è¿”å›åŸå§‹è¡¨ã€‚")
        print("ğŸ” å½“å‰è¡¨å¤´ä¸ºï¼š", list(df.columns))
        display(df.head())
        # è¿™é‡Œç›´æ¥è¿”å›åŸè¡¨ï¼Œä¸åšä»»ä½•ä¿®æ”¹ï¼ˆè¿åºå·éƒ½ä¸åŠ ï¼‰
        return df

    # ===== âœ… æƒ…å†µ 1ï¼šå­˜åœ¨æ—¥æœŸåˆ— â†’ æŒ‰æ—¥æœŸæ’åºåä¿ç•™æœ€æ–° =====
    if date_col in df.columns:
        print(f"âœ… ä½¿ç”¨æ—¥æœŸåˆ—ã€{date_col}ã€‘è¿›è¡Œæ’åºå»é‡ï¼ˆä¿ç•™æœ€æ–°ï¼‰")
        df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
        df = df.sort_values(by=date_col)
        df = df.drop_duplicates(subset=dedup_cols, keep="last").copy()

    # ===== âœ… æƒ…å†µ 2ï¼šä¸å­˜åœ¨æ—¥æœŸåˆ— â†’ ç›´æ¥æŒ‰åŸé¡ºåºä¿ç•™æœ€åä¸€è¡Œ =====
    else:
        print(f"âš ï¸ æœªå‘ç°æ—¥æœŸåˆ—ã€{date_col}ã€‘ï¼Œæ”¹ä¸ºç›´æ¥ä¿ç•™æœ€åä¸€æ¡è®°å½•")
        df = df.drop_duplicates(subset=dedup_cols, keep="last").copy()

    # ===== âœ… åˆ é™¤ã€å—ç†å·ã€‘=====
    if "å—ç†å·" in df.columns:
        df = df.drop(columns=["å—ç†å·"])

    # ===== âœ… æ·»åŠ ã€åºå·ã€‘=====
    # df.insert(0, "åºå·", range(1, len(df) + 1))
    if "åºå·" not in df.columns:
        df.insert(0, "åºå·", range(1, len(df) + 1))
    else:
        print("â„¹ï¸ å·²å­˜åœ¨ã€åºå·ã€‘åˆ—ï¼Œè·³è¿‡è‡ªåŠ¨ç”Ÿæˆ")

    print("âœ… å»é‡åè¡Œæ•°ï¼ˆæœ€ç»ˆä¿ç•™è§„åˆ™ç”Ÿæ•ˆï¼‰ï¼š", len(df))
    # display(df.head())

    return df

# def step1_nmpa_filter_by_quarter(
#     input_path: str,
#     sheet_name: str = "æ•°æ®è¯¦æƒ…",
#     approval_date_col: str = "æœ€æ–°æ‰¹å‡†æ—¥æœŸ",
#     drug_name_col: str = "é€šç”¨å",
#     year: int = None,          # âœ… ä¾‹å¦‚ 2024
#     quarter: str = "Q4"       # âœ… "Q1" / "Q2" / "Q3" / "Q4"
# ):
#     """
#     NMPA ä¸“ç”¨ï¼ˆæŒ‰è‡ªç„¶å­£åº¦ç­›é€‰ï¼‰ï¼š
#     1ï¼‰è¯»å–ã€æ•°æ®è¯¦æƒ…ã€‘sheet
#     2ï¼‰æŒ‰ã€æŒ‡å®šå¹´ä»½ + å­£åº¦ï¼ˆQ1~Q4ï¼‰ã€‘ç­›é€‰æ‰¹å‡†è¯å“
#     3ï¼‰ç›¸åŒã€é€šç”¨åã€‘â†’ ä½¿ç”¨ç›¸åŒã€åºå·ã€‘ï¼ˆä¸åŒè§„æ ¼å…±ç”¨ï¼‰
#     """

#     if year is None:
#         raise ValueError("âŒ å¿…é¡»æ˜¾å¼æŒ‡å®š yearï¼Œä¾‹å¦‚ year=2024")

#     quarter = quarter.upper()
#     if quarter not in ["Q1", "Q2", "Q3", "Q4"]:
#         raise ValueError("âŒ quarter åªèƒ½æ˜¯ï¼š'Q1', 'Q2', 'Q3', 'Q4'")

#     # ===== âœ… 1ï¸âƒ£ è¯»å–æ•°æ® =====
#     df = pd.read_excel(input_path, sheet_name=sheet_name)
#     print("âœ… NMPA åŸå§‹æ•°æ®è¡Œæ•°ï¼š", len(df))

#     # ===== âœ… 2ï¸âƒ£ æ£€æŸ¥å…³é”®åˆ— =====
#     for col in [approval_date_col, drug_name_col]:
#         if col not in df.columns:
#             raise ValueError(
#                 f"âŒ æ‰¾ä¸åˆ°åˆ—ï¼š{col}ï¼Œè¯·æ£€æŸ¥ NMPA è¡¨å¤´ã€‚å½“å‰åˆ—ï¼š{list(df.columns)}"
#             )

#     # ===== âœ… 3ï¸âƒ£ æ—¶é—´è½¬æ¢ =====
#     df[approval_date_col] = pd.to_datetime(df[approval_date_col], errors="coerce")

#     # ===== âœ… 4ï¸âƒ£ å®šä¹‰å­£åº¦èµ·æ­¢æ—¥æœŸ =====
#     if quarter == "Q1":
#         start_date = pd.Timestamp(year=year, month=1, day=1)
#         end_date   = pd.Timestamp(year=year, month=3, day=31)
#     elif quarter == "Q2":
#         start_date = pd.Timestamp(year=year, month=4, day=1)
#         end_date   = pd.Timestamp(year=year, month=6, day=30)
#     elif quarter == "Q3":
#         start_date = pd.Timestamp(year=year, month=7, day=1)
#         end_date   = pd.Timestamp(year=year, month=9, day=30)
#     else:  # Q4
#         start_date = pd.Timestamp(year=year, month=10, day=1)
#         end_date   = pd.Timestamp(year=year, month=12, day=31)

#     # ===== âœ… 5ï¸âƒ£ æŒ‰å­£åº¦ç­›é€‰ =====
#     df_q = df[
#         (df[approval_date_col] >= start_date) &
#         (df[approval_date_col] <= end_date)
#     ].copy()

#     print(f"âœ… ç­›é€‰åŒºé—´ï¼š{start_date.date()} ~ {end_date.date()}")
#     print(f"âœ… è¯¥å­£åº¦æ‰¹å‡†è¡Œæ•°ï¼š", len(df_q))

#     # ===== âœ… 6ï¸âƒ£ ç›¸åŒé€šç”¨å â†’ åŒä¸€åºå· =====
#     unique_drugs = (
#         df_q[drug_name_col]
#         .dropna()
#         .drop_duplicates()
#         .reset_index(drop=True)
#     )

#     drug_to_id = {
#         name: idx + 1
#         for idx, name in unique_drugs.items()
#     }

#     df_q["åºå·"] = df_q[drug_name_col].map(drug_to_id)

#     print("âœ… NMPA æŒ‰å­£åº¦å¤„ç†å®Œæˆï¼Œæ·»åŠ åºå·åè¡Œæ•°ï¼š", len(df_q))
#     # display(df_q.head())

#     return df_q

def step1_nmpa_filter_by_quarter(
    input_path: str,
    sheet_name: str = "æ•°æ®è¯¦æƒ…",
    approval_date_col: str = "æœ€æ–°æ‰¹å‡†æ—¥æœŸ",
    drug_name_col: str = "é€šç”¨å",
    dosage_col: str = "å‰‚å‹",          # â­ æ–°å¢å­—æ®µï¼šç”¨äºå»é‡
    year: int = None,
    quarter: str = "Q4"
):
    """
    NMPA ä¸“ç”¨ï¼ˆæŒ‰è‡ªç„¶å­£åº¦ç­›é€‰ï¼‰ï¼š
    1ï¼‰ç­›é€‰å­£åº¦æ‰¹å‡†è¯å“
    2ï¼‰æŒ‰ã€é€šç”¨å + "æŒè¯å•†(NMPA)"ã€‘åˆ†ç»„ â†’ å…±ç”¨åŒä¸€åºå·
    """

    if year is None:
        raise ValueError("âŒ å¿…é¡»æ˜¾å¼æŒ‡å®š yearï¼Œä¾‹å¦‚ year=2024")

    quarter = quarter.upper()
    if quarter not in ["Q1", "Q2", "Q3", "Q4"]:
        raise ValueError("âŒ quarter åªèƒ½æ˜¯ï¼š'Q1', 'Q2', 'Q3', 'Q4'")

    # ===== 1ï¸âƒ£ è¯»å– =====
    df = pd.read_excel(input_path, sheet_name=sheet_name)
    print("âœ… NMPA åŸå§‹æ•°æ®è¡Œæ•°ï¼š", len(df))

    # ===== 2ï¸âƒ£ æ£€æŸ¥å­—æ®µ =====
    for col in [approval_date_col, drug_name_col, dosage_col]:
        if col not in df.columns:
            raise ValueError(
                f"âŒ æ‰¾ä¸åˆ°åˆ—ï¼š{col}ï¼ˆå½“å‰åˆ—ï¼š{list(df.columns)}ï¼‰"
            )

    # ===== 3ï¸âƒ£ æ—¶é—´æ ¼å¼å¤„ç† =====
    df[approval_date_col] = pd.to_datetime(df[approval_date_col], errors="coerce")

    # ===== 4ï¸âƒ£ è®¡ç®—å­£åº¦èµ·æ­¢ =====
    if quarter == "Q1":
        start_date = pd.Timestamp(year=year, month=1, day=1)
        end_date = pd.Timestamp(year=year, month=3, day=31)
    elif quarter == "Q2":
        start_date = pd.Timestamp(year=year, month=4, day=1)
        end_date = pd.Timestamp(year=year, month=6, day=30)
    elif quarter == "Q3":
        start_date = pd.Timestamp(year=year, month=7, day=1)
        end_date = pd.Timestamp(year=year, month=9, day=30)
    else:  # Q4
        start_date = pd.Timestamp(year=year, month=10, day=1)
        end_date = pd.Timestamp(year=year, month=12, day=31)

    # ===== 5ï¸âƒ£ æŒ‰å­£åº¦ç­›é€‰ =====
    df_q = df[
        (df[approval_date_col] >= start_date) &
        (df[approval_date_col] <= end_date)
    ].copy()

    print(f"ğŸ“Œ ç­›é€‰åŒºé—´ï¼š{start_date.date()} ~ {end_date.date()}")
    print(f"ğŸ“Œ å­£åº¦å†…æ‰¹å‡†è®°å½•æ•°ï¼š{len(df_q)}")

    # ===== 6ï¸âƒ£ â­ æŒ‰ã€é€šç”¨å + å‰‚å‹ã€‘å»é‡ç”Ÿæˆåºå· =====
    # unique_pairs = (
    #     df_q[[drug_name_col, dosage_col,"æŒè¯å•†(NMPA)"]]
    #     .dropna()
    #     .drop_duplicates()
    #     .reset_index(drop=True)
    # )

    # # ç”Ÿæˆåºå·
    # drug_to_id = {
    #     (row[drug_name_col], row[dosage_col]): idx + 1
    #     for idx, row in unique_pairs.iterrows()
    # }

    # df_q["åºå·"] = df_q.apply(
    # lambda r: drug_to_id.get((r[drug_name_col], r[dosage_col])),
    # axis=1
    # )

    # print(f"âœ… NMPA æŒ‰ã€é€šç”¨å + å‰‚å‹ã€‘æ·»åŠ åºå·åè¡Œæ•°ï¼š{len(df_q)}")
    # ===== 6ï¸âƒ£ â­ æŒ‰ã€é€šç”¨å + å‰‚å‹ + æŒè¯å•†(NMPA)ã€‘å»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¡ =====
    dedup_cols = [drug_name_col, dosage_col, "æŒè¯å•†(NMPA)"]

    before = len(df_q)

    df_q = (
        df_q.sort_values(approval_date_col)  # å¦‚éœ€è¦ä¿æŒæ—¶é—´é¡ºåº
            .drop_duplicates(subset=dedup_cols, keep="first")
            .reset_index(drop=True)
    )

    after = len(df_q)
    print(f"âœ… NMPA å»é‡å®Œæˆï¼šåˆ é™¤ {before - after} æ¡é‡å¤è®°å½•ï¼ˆåŸºäº {dedup_cols}ï¼‰")

    # ===== âœ… æ·»åŠ ã€åºå·ã€‘åˆ— =====
    if "åºå·" not in df_q.columns:
        df_q.insert(0, "åºå·", range(1, len(df_q) + 1))
        print(f"âœ… å·²ä¸º NMPA ç»“æœæ·»åŠ ã€åºå·ã€‘åˆ—ï¼Œè¡Œæ•°ï¼š{len(df_q)}")
    else:
        print("â„¹ï¸ æ£€æµ‹åˆ°å·²æœ‰ã€åºå·ã€‘åˆ—ï¼Œä¿ç•™åŸæœ‰åºå·")

    return df_q
# def step1_fda_dedup_and_add_id(
#     input_path: str,
#     sheet_name: str = "ç›®æ ‡è¯å“",
# ):
#     """
#     FDA ä¸“ç”¨ï¼ˆå½“å‰è§„åˆ™ï¼‰ï¼š
#     1ï¼‰è¯»å–ã€ç›®æ ‡è¯å“ã€‘sheet
#     2ï¼‰ä¸åšä»»ä½•å»é‡
#     3ï¼‰æŒ‰åŸå§‹é¡ºåºä»ä¸Šåˆ°ä¸‹ç›´æ¥æ·»åŠ ã€åºå·ã€‘
#     """

#     df = pd.read_excel(input_path, sheet_name=sheet_name)

#     print("âœ… FDA åŸå§‹æ•°æ®è¡Œæ•°ï¼š", len(df))
#     # display(df.head())

#     # ===== âœ… ç›´æ¥æŒ‰è¡Œå·æ·»åŠ ã€åºå·ã€‘=====
#     if "åºå·" not in df.columns:
#         df.insert(0, "åºå·", range(1, len(df) + 1))
#     else:
#         print("â„¹ï¸ FDA è¡¨ä¸­å·²å­˜åœ¨ã€åºå·ã€‘åˆ—ï¼Œè·³è¿‡è‡ªåŠ¨ç”Ÿæˆ")

#     print("âœ… FDA æ·»åŠ åºå·åè¡Œæ•°ï¼š", len(df))
#     # display(df.head())

#     return df

def step1_fda_dedup_and_add_id(
    input_path: str,
    sheet_name: str = "ç›®æ ‡è¯å“",
    dedup_cols=["æ´»æ€§æˆåˆ†(ä¸­æ–‡)", "ç”³è¯·æœºæ„","å‰‚å‹"]
):
    """
    FDA ä¸“ç”¨ï¼ˆæœ€æ–°è§„åˆ™ï¼‰ï¼š
    1ï¼‰è¯»å–ã€ç›®æ ‡è¯å“ã€‘sheet
    2ï¼‰æŒ‰ã€æ´»æ€§æˆåˆ†ï¼ˆä¸­æ–‡ï¼‰ + æŒè¯å•†(NMPA)ã€‘å»é‡ï¼ˆä¿ç•™æœ€åä¸€æ¡ï¼‰
    3ï¼‰æŒ‰æœ€ç»ˆé¡ºåºæ·»åŠ ã€åºå·ã€‘
    """

    df = pd.read_excel(input_path, sheet_name=sheet_name)

    print("âœ… FDA åŸå§‹æ•°æ®è¡Œæ•°ï¼š", len(df))

    # ===== 1ï¸âƒ£ æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨ =====
    missing_cols = [c for c in dedup_cols if c not in df.columns]
    if missing_cols:
        raise ValueError(
            f"âŒ FDA è¡¨ç¼ºå°‘ä»¥ä¸‹å»é‡å­—æ®µï¼š{missing_cols}\n"
            f"å½“å‰è¡¨å¤´ï¼š{list(df.columns)}"
        )

    # ===== 2ï¸âƒ£ å»é‡ï¼ˆä¿ç•™æœ€åä¸€æ¡è®°å½•ï¼‰=====
    df_dedup = df.drop_duplicates(subset=dedup_cols, keep="last").copy()

    print(f"ğŸ” FDA æŒ‰ {dedup_cols} å»é‡åè¡Œæ•°ï¼š{len(df_dedup)}")

    # ===== 3ï¸âƒ£ æ·»åŠ åºå· =====
    df_dedup.insert(0, "åºå·", range(1, len(df_dedup) + 1))

    print("âœ… FDA æ·»åŠ åºå·åè¡Œæ•°ï¼š", len(df_dedup))

    return df_dedup

# def build_classify_mapping():
#     mapping_data = [
#         ["ç”Ÿç‰©åˆ¶å“", "æŠ—ä½“", "BIO", "Antibody"],
#         ["åŒ–å­¦è¯å“", "å…¶ä»–", "SMD", "SMD"],
#         ["ç”Ÿç‰©åˆ¶å“", "å…¶ä»–", "BIO", "BIO"],
#         ["ç”Ÿç‰©åˆ¶å“", "ç–«è‹—", "BIO", "Vaccine"],
#         ["ç”Ÿç‰©åˆ¶å“", "ç»†èƒç–—æ³•", "CGT", "CGT"],
#         ["ä¸­è¯", "ä¸­æˆè¯", "TCM", "TCM"],
#         ["ç”Ÿç‰©åˆ¶å“", "åŸºå› ç–—æ³•", "CGT", "CGT"],
#         ["åŒ–å­¦è¯å“", "å¤šè‚½", "Polypeptide", "Polypeptide"],
#         ["åŒ–å­¦è¯å“", "æ ¸é…¸", "SMD", "RNA"],
#         ["ç”Ÿç‰©åˆ¶å“", "å¤šè‚½", "Polypeptide", "Polypeptide"],
#         ["ä¸­è¯", "ä¸­è¯å•ä½“", "TCM", "TCM"],
#         ["ç”Ÿç‰©åˆ¶å“", "æ ¸é…¸", "BIO", "RNA"],
#     ]

#     df_map = pd.DataFrame(
#         mapping_data,
#         columns=["è¯å“ç±»åˆ«ä¸€", "è¯å“ç±»åˆ«äºŒ", "ç±»åˆ«(ç²—åˆ†)", "è¯¦ç»†åˆ—ï¼ˆç»†åˆ†ï¼‰"]
#     )

#     return df_map
def build_classify_mapping_from_json():
    """
    âœ… è‡ªåŠ¨ä»ç¨‹åºåŒçº§ç›®å½•è¯»å– rules_config.json
    """
    config_path = os.path.join(get_base_dir(), "rules_config.json")

    if not os.path.exists(config_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°è§„åˆ™é…ç½®æ–‡ä»¶ï¼š{config_path}")

    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)

    mapping_data = config["classification_mapping"]
    df_map = pd.DataFrame(mapping_data)

    return df_map

def step2_add_class_and_save(
    df,
    df_map,
    output_classified_path: str
):
    df_with_class = df.merge(
        df_map,
        on=["è¯å“ç±»åˆ«ä¸€", "è¯å“ç±»åˆ«äºŒ"],
        how="left"
    )
    col = df_with_class["ç±»åˆ«(ç²—åˆ†)"].astype(str).str.strip()
    missing = df_with_class[
    df_with_class["ç±»åˆ«(ç²—åˆ†)"].astype(str).str.strip().isin(["", "nan", "NaN", "None"])
    ]

    
    
    # âœ… æœªåŒ¹é…æ£€æŸ¥
    # col = df_with_class["ç±»åˆ«(ç²—åˆ†)"].astype(str).str.strip()

    # missing = df_with_class[
    #     col.isna() | col.eq("") | col.eq("nan") | col.eq("NaN") | col.eq("None")
    # ]
    # missing = df_with_class[
    #     df_with_class["ç±»åˆ«(ç²—åˆ†)"]
    #     .astype(str)
    #     .str.strip()
    #     .isin(["", "nan", "NaN", "None"])
    # ]
    # missing = df_with_class[df_with_class["ç±»åˆ«(ç²—åˆ†)"].isna()]
    if len(missing) > 0:
        print("âš ï¸ å‘ç°æœªåŒ¹é…åˆ†ç±»çš„è®°å½•ï¼š")
        print(missing.shape)
        display(missing)
        # display(
        #     missing.drop_duplicates(subset=["è¯å“ç±»åˆ«ä¸€", "è¯å“ç±»åˆ«äºŒ"])
        # )
        # display(missing[["è¯å“ç±»åˆ«ä¸€", "è¯å“ç±»åˆ«äºŒ"]].drop_duplicates())
    else:
        print("âœ… æ‰€æœ‰è®°å½•å·²æˆåŠŸåŒ¹é…åˆ†ç±»")
    df_with_class.loc[col.isin(["", "nan", "NaN", "None"]), "ç±»åˆ«(ç²—åˆ†)"] = "Others"
    # display(df_with_class.head())
    # ========== ç»™ç»†åˆ†ç±»è¡¥ Others ==========
    fine_col = df_with_class["è¯¦ç»†åˆ—ï¼ˆç»†åˆ†ï¼‰"].astype(str).str.strip()

    df_with_class.loc[
        fine_col.isin(["", "nan", "NaN", "None"]),
        "è¯¦ç»†åˆ—ï¼ˆç»†åˆ†ï¼‰"
    ] = "Others"
    # ======================================
    # âœ… åªä¿å­˜è¿™ä¸€ä»½
    df_with_class.to_excel(output_classified_path, index=False)
    print(f"âœ… åˆ†ç±»æ˜ç»†è¡¨å·²ä¿å­˜ï¼š{output_classified_path}")

    return df_with_class

# def step3_print_statistics(df):

#     # ===== âœ… å†…éƒ¨å°å·¥å…·ï¼šç»™ç»Ÿè®¡è¡¨æ·»åŠ  Total è¡Œ =====
#     def add_total_row(stat_df, name_col="ç±»åˆ«", count_col="æ•°é‡"):
#         total_value = stat_df[count_col].sum()
#         total_row = pd.DataFrame({
#             name_col: ["Total"],
#             count_col: [total_value]
#         })
#         stat_df_with_total = pd.concat([stat_df, total_row], ignore_index=True)
#         return stat_df_with_total

#     # ===============================
#     # âœ… ä¸€ã€æŒ‰ã€è¯å“ç±»åˆ«ä¸€ã€‘ç»Ÿè®¡
#     # ===============================
#     print("âœ… ä¸€ã€æŒ‰ã€è¯å“ç±»åˆ«ä¸€ã€‘ç»Ÿè®¡ï¼š")
#     stat_cat1 = df["è¯å“ç±»åˆ«ä¸€"].value_counts().reset_index()
#     stat_cat1.columns = ["è¯å“ç±»åˆ«ä¸€", "æ•°é‡"]

#     stat_cat1 = add_total_row(
#         stat_cat1,
#         name_col="è¯å“ç±»åˆ«ä¸€",
#         count_col="æ•°é‡"
#     )

#     display(stat_cat1)

#     # ===============================
#     # âœ… äºŒã€æŒ‰ã€ç²—åˆ†ç±»ã€‘ç»Ÿè®¡
#     # ===============================
#     print("âœ… äºŒã€æŒ‰ã€ç²—åˆ†ç±»ã€‘ç»Ÿè®¡ï¼š")
#     stat_coarse = df["ç±»åˆ«(ç²—åˆ†)"].value_counts().reset_index()
#     stat_coarse.columns = ["ç±»åˆ«(ç²—åˆ†)", "æ•°é‡"]

#     stat_coarse = add_total_row(
#         stat_coarse,
#         name_col="ç±»åˆ«(ç²—åˆ†)",
#         count_col="æ•°é‡"
#     )

#     display(stat_coarse)

#     # ===============================
#     # âœ… ä¸‰ã€æŒ‰ã€ç»†åˆ†ç±»ã€‘ç»Ÿè®¡
#     # ===============================
#     print("âœ… ä¸‰ã€æŒ‰ã€ç»†åˆ†ç±»ã€‘ç»Ÿè®¡ï¼š")
#     stat_fine = df["è¯¦ç»†åˆ—ï¼ˆç»†åˆ†ï¼‰"].value_counts().reset_index()
#     stat_fine.columns = ["è¯¦ç»†åˆ—ï¼ˆç»†åˆ†ï¼‰", "æ•°é‡"]

#     stat_fine = add_total_row(
#         stat_fine,
#         name_col="è¯¦ç»†åˆ—ï¼ˆç»†åˆ†ï¼‰",
#         count_col="æ•°é‡"
#     )

#     display(stat_fine)

#     return stat_cat1, stat_coarse, stat_fine

def step3_print_statistics(df, show: bool = True):

    def add_total_row(stat_df, name_col="ç±»åˆ«", count_col="æ•°é‡"):
        total_value = stat_df[count_col].sum()
        total_row = pd.DataFrame({
            name_col: ["Total"],
            count_col: [total_value]
        })
        return pd.concat([stat_df, total_row], ignore_index=True)

    stat_cat1 = None
    stat_coarse = None
    stat_fine = None

    # ===============================
    # âœ… ä¸€ã€æŒ‰ã€è¯å“ç±»åˆ«ä¸€ã€‘ç»Ÿè®¡
    # ===============================
    if "è¯å“ç±»åˆ«ä¸€" in df.columns:
        stat_cat1 = df["è¯å“ç±»åˆ«ä¸€"].value_counts().reset_index()
        stat_cat1.columns = ["è¯å“ç±»åˆ«ä¸€", "æ•°é‡"]
        stat_cat1 = add_total_row(stat_cat1, "è¯å“ç±»åˆ«ä¸€", "æ•°é‡")

        if show:
            print("âœ… ä¸€ã€æŒ‰ã€è¯å“ç±»åˆ«ä¸€ã€‘ç»Ÿè®¡ï¼š")
            display(stat_cat1)
    else:
        if show:
            print("âš ï¸ è·³è¿‡ã€è¯å“ç±»åˆ«ä¸€ã€‘ç»Ÿè®¡ï¼šå½“å‰ DataFrame ä¸­ä¸å­˜åœ¨è¯¥åˆ—")

    # ===============================
    # âœ… äºŒã€æŒ‰ã€ç²—åˆ†ç±»ã€‘ç»Ÿè®¡
    # ===============================
    if "ç±»åˆ«(ç²—åˆ†)" in df.columns:
        stat_coarse = df["ç±»åˆ«(ç²—åˆ†)"].value_counts().reset_index()
        stat_coarse.columns = ["ç±»åˆ«(ç²—åˆ†)", "æ•°é‡"]
        stat_coarse = add_total_row(stat_coarse, "ç±»åˆ«(ç²—åˆ†)", "æ•°é‡")

        if show:
            print("âœ… äºŒã€æŒ‰ã€ç²—åˆ†ç±»ã€‘ç»Ÿè®¡ï¼š")
            display(stat_coarse)
    else:
        if show:
            print("âš ï¸ è·³è¿‡ã€ç²—åˆ†ç±»ã€‘ç»Ÿè®¡ï¼šå½“å‰ DataFrame ä¸­ä¸å­˜åœ¨åˆ—ã€ç±»åˆ«(ç²—åˆ†)ã€‘")

    # ===============================
    # âœ… ä¸‰ã€æŒ‰ã€ç»†åˆ†ç±»ã€‘ç»Ÿè®¡
    # ===============================
    if "è¯¦ç»†åˆ—ï¼ˆç»†åˆ†ï¼‰" in df.columns:
        stat_fine = df["è¯¦ç»†åˆ—ï¼ˆç»†åˆ†ï¼‰"].value_counts().reset_index()
        stat_fine.columns = ["è¯¦ç»†åˆ—ï¼ˆç»†åˆ†ï¼‰", "æ•°é‡"]
        stat_fine = add_total_row(stat_fine, "è¯¦ç»†åˆ—ï¼ˆç»†åˆ†ï¼‰", "æ•°é‡")

        if show:
            print("âœ… ä¸‰ã€æŒ‰ã€ç»†åˆ†ç±»ã€‘ç»Ÿè®¡ï¼š")
            display(stat_fine)
    else:
        if show:
            print("âš ï¸ è·³è¿‡ã€ç»†åˆ†ç±»ã€‘ç»Ÿè®¡ï¼šå½“å‰ DataFrame ä¸­ä¸å­˜åœ¨åˆ—ã€è¯¦ç»†åˆ—ï¼ˆç»†åˆ†ï¼‰ã€‘")

    return stat_cat1, stat_coarse, stat_fine

# def build_disease_area_mapping():
#     return {
#         "Oncology": "è‚¿ç˜¤",
#         "Hematology": "è¡€æ¶²",
#         "Infectious": "æ„ŸæŸ“",
#         "Respiratory": "å‘¼å¸",
#         "Gastrointestinal": "æ¶ˆåŒ–",
#         "Dermatology": "çš®è‚¤",
#         "Rare disease": "ç½•è§ç–¾ç—…",
#         "Immunology": "å…ç–«",
#         "Other": "å…¶ä»–"
#     }

def load_disease_area_mapping_from_json():
    config_path = os.path.join(get_base_dir(), "rules_config.json")

    if not os.path.exists(config_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°è§„åˆ™é…ç½®æ–‡ä»¶ï¼š{config_path}")

    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)

    return config["disease_area_mapping"]

def step4_statistics_by_disease_area(df, disease_col: str = "å‚è€ƒç–¾ç—…é¢†åŸŸ", show: bool = True):

    mapping = load_disease_area_mapping_from_json()

    if disease_col not in df.columns:
        if show:
            print(f"âš ï¸ è·³è¿‡ç–¾ç—…é¢†åŸŸç»Ÿè®¡ï¼šæ‰¾ä¸åˆ°åˆ—ã€{disease_col}ã€‘")
        return None

    stat_rows = []

    for eng, zh in mapping.items():
        count = df[disease_col].astype(str).str.contains(zh, na=False).sum()
        stat_rows.append([eng, zh, count])

    stat_df = pd.DataFrame(
        stat_rows,
        columns=["ç–¾ç—…é¢†åŸŸ(è‹±æ–‡)", "ç–¾ç—…é¢†åŸŸ(ä¸­æ–‡)", "æ•°é‡"]
    )

    total_value = stat_df["æ•°é‡"].sum()
    total_row = pd.DataFrame([["Total", "Total", total_value]],
                             columns=stat_df.columns)
    stat_df = pd.concat([stat_df, total_row], ignore_index=True)

    if show:
        print("âœ… æŒ‰ã€å‚è€ƒç–¾ç—…é¢†åŸŸã€‘ç»Ÿè®¡ç»“æœï¼š")
        display(stat_df)

    return stat_df

# def step5_statistics_by_target(df, target_col: str = "é¶ç‚¹", show: bool = True):

#     if target_col not in df.columns:
#         if show:
#             print(f"âš ï¸ è·³è¿‡é¶ç‚¹ç»Ÿè®¡ï¼šå½“å‰ DataFrame ä¸­ä¸å­˜åœ¨åˆ—ã€{target_col}ã€‘")
#         return (
#             pd.DataFrame(columns=["é¶ç‚¹", "æ•°é‡"]),
#             pd.DataFrame(columns=["é¶ç‚¹", "æ•°é‡"])
#         )

#     s = (
#         df[target_col]
#         .astype(str)
#         .fillna("")
#         .str.strip()
#     )

#     s = s[s != ""]

#     if s.empty:
#         if show:
#             print("âš ï¸ é¶ç‚¹åˆ—ä¸ºç©ºæˆ–ä»…åŒ…å«ç©ºå€¼ï¼Œè¿”å›ç©ºç»Ÿè®¡è¡¨ã€‚")
#         return (
#             pd.DataFrame(columns=["é¶ç‚¹", "æ•°é‡"]),
#             pd.DataFrame(columns=["é¶ç‚¹", "æ•°é‡"])
#         )

#     vc = s.value_counts()
#     detail_df = vc.reset_index()
#     detail_df.columns = ["é¶ç‚¹", "æ•°é‡"]

#     top_k = 10
#     top_df = detail_df.head(top_k).copy()
#     others_count = detail_df["æ•°é‡"].iloc[top_k:].sum()

#     summary_rows = []

#     for _, row in top_df.iterrows():
#         summary_rows.append([row["é¶ç‚¹"], row["æ•°é‡"]])

#     if others_count > 0:
#         summary_rows.append(["others", others_count])

#     summary_df = pd.DataFrame(summary_rows, columns=["é¶ç‚¹", "æ•°é‡"])

#     total_row = pd.DataFrame(
#         [["Total", summary_df["æ•°é‡"].sum()]],
#         columns=["é¶ç‚¹", "æ•°é‡"]
#     )
#     summary_df = pd.concat([summary_df, total_row], ignore_index=True)

#     if show:
#         print("âœ… æŒ‰ã€é¶ç‚¹ã€‘ç»Ÿè®¡ç»“æœï¼ˆTop10 + othersï¼‰ï¼š")
#         display(summary_df)

#     return detail_df, summary_df
def step5_statistics_by_target(df, target_col: str = "é¶ç‚¹", show: bool = True):

    if target_col not in df.columns:
        if show:
            print(f"âš ï¸ è·³è¿‡ï¼šä¸å­˜åœ¨åˆ—ã€{target_col}ã€‘")
        empty = pd.DataFrame(columns=["é¶ç‚¹", "æ•°é‡"])
        return empty, empty

    # ä¸‰åˆ—ç”¨äºåˆ¤æ–­ç©ºå€¼
    cols_check = [target_col, "è¯å“ç±»åˆ«ä¸€", "è¯å“ç±»åˆ«äºŒ"]

    for col in cols_check:
        if col not in df.columns:
            raise KeyError(f"âŒ DataFrame ç¼ºå°‘å¿…è¦åˆ—ï¼š{col}")

    # ç»Ÿä¸€æ¸…æ´—
    cleaned = df[cols_check].astype(str).apply(lambda c: c.str.strip())
    empty_vals = ["", "nan", "NaN", "None"]

    # === åˆ¤æ–­ä¸‰åˆ—æ˜¯å¦å…¨éƒ¨ä¸ºç©º ===
    mask_all_empty = cleaned.apply(lambda row: all(v in empty_vals for v in row), axis=1)

    # -------------------------------
    # âœ…ï¼ˆæ–°å¢ï¼‰æ‰“å°æ€»è¡Œæ•°ã€æœ‰æ•ˆé¶ç‚¹è¡Œæ•°
    # -------------------------------
    total_rows = len(df)
    rows_no_target = mask_all_empty.sum()
    rows_valid = total_rows - rows_no_target

    if show:
        print("ğŸ“Š é¶ç‚¹ç»Ÿè®¡åŸºç¡€ä¿¡æ¯ï¼š")
        print(f"  â€¢ æ€»è¡Œæ•°ï¼š{total_rows}")
        print(f"  â€¢ ä¸‰åˆ—çš†ä¸ºç©º â†’ æ— æœ‰æ•ˆé¶ç‚¹ çš„è¡Œæ•°ï¼š{rows_no_target}")
        print(f"  â€¢ è¿›å…¥é¶ç‚¹ç»Ÿè®¡çš„æœ‰æ•ˆè¡Œæ•°ï¼š{rows_valid}")
        print("-" * 50)

    # è¿‡æ»¤æœ‰æ•ˆ
    # === ä¿®å¤ï¼šé¶ç‚¹ä¸ºç©ºä½†è¡Œæœ‰æ•ˆçš„ä¹Ÿå½’å…¥ Others ===

    valid_df = df[~mask_all_empty].copy()
    valid_df[target_col] = valid_df[target_col].astype(str).str.strip()
    valid_df.loc[
        valid_df[target_col].isin(["", "nan", "NaN", "None"]),
        target_col
    ] = "others"

    if valid_df.empty:
        if show:
            print("âš ï¸ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆé¶ç‚¹ä¿¡æ¯ï¼Œè¿”å›ç©ºè¡¨")
        empty = pd.DataFrame(columns=["é¶ç‚¹", "æ•°é‡"])
        return empty, empty

    # ä½¿ç”¨é¶ç‚¹åˆ—
    s = valid_df[target_col].astype(str).str.strip()
    s = s[~s.isin(empty_vals)]

    # value_counts
    vc = s.value_counts()
    detail_df = vc.rename_axis("é¶ç‚¹").reset_index(name="æ•°é‡")

    # Top10 + others
    top_k = 10
    if len(detail_df) > top_k:
        top_df = detail_df.head(top_k)
        others_count = detail_df["æ•°é‡"].iloc[top_k:].sum()
        summary_df = pd.concat(
            [top_df, pd.DataFrame([["others", others_count]], columns=["é¶ç‚¹", "æ•°é‡"])]
        )
    else:
        summary_df = detail_df.copy()

    # æ·»åŠ  Total è¡Œ
    summary_df.loc[len(summary_df)] = ["Total", summary_df["æ•°é‡"].sum()]

    if show:
        display(summary_df)

    return detail_df, summary_df

# def step5_statistics_by_target(df, target_col: str = "é¶ç‚¹", show: bool = True):

#     # 1. åˆ—ä¸å­˜åœ¨
#     if target_col not in df.columns:
#         if show:
#             print(f"âš ï¸ è·³è¿‡ï¼šä¸å­˜åœ¨åˆ—ã€{target_col}ã€‘")
#         empty = pd.DataFrame(columns=["é¶ç‚¹", "æ•°é‡"])
#         return empty, empty

#     # 2. å…ˆ dropnaï¼Œå†è½¬ strï¼ˆé¡ºåºå¾ˆé‡è¦ï¼ï¼‰
#     s = (
#         df[target_col]
#         .dropna()             # å»æ‰ NaNï¼ˆå…³é”®æ­¥éª¤ï¼‰
#         .astype(str)          # è½¬æˆå­—ç¬¦ä¸²
#         .str.strip()          # å»æ‰ä¸¤ç«¯ç©ºç™½
#     )

#     # 3. å»æ‰ç©ºå­—ç¬¦ä¸²ã€"nan"ã€"None" ç­‰è„å€¼
#     s = s[~s.isin(["", "nan", "None", "NaN"])]
#     if s.empty:
#         if show:
#             print("âš ï¸ é¶ç‚¹åˆ—ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ")
#         empty = pd.DataFrame(columns=["é¶ç‚¹", "æ•°é‡"])
#         return empty, empty

#     # 4. value_counts ç»Ÿè®¡
#     vc = s.value_counts(dropna=True)
#     detail_df = vc.rename_axis("é¶ç‚¹").reset_index(name="æ•°é‡")

#     # 5. ç”Ÿæˆ top10 + others
#     top_k = 10
#     if len(detail_df) > top_k:
#         top_df = detail_df.head(top_k)
#         others_count = detail_df["æ•°é‡"].iloc[top_k:].sum()
#         summary_df = pd.concat(
#             [top_df, pd.DataFrame([["others", others_count]], columns=["é¶ç‚¹", "æ•°é‡"])]
#         )
#     else:
#         summary_df = detail_df.copy()

#     # 6. Total è¡Œ
#     total = summary_df["æ•°é‡"].sum()
#     summary_df.loc[len(summary_df)] = ["Total", total]

#     if show:
#         display(summary_df)

#     return detail_df, summary_df

def save_all_stats_to_one_sheet(
    output_file,
    stat_cat1,
    stat_coarse,
    stat_fine,
    stat_disease_area,
    summary_target,
    detail_target,
    sheet_name="æ‰€æœ‰ç»Ÿè®¡æ±‡æ€»"
):
    """
    æŠŠ Step 3-5 çš„æ‰€æœ‰ç»Ÿè®¡ç»“æœï¼ŒæŒ‰åŒºå—å†™å…¥åŒä¸€ä¸ª Sheetã€‚
    âœ… è‡ªåŠ¨è·³è¿‡ None æˆ–ç©º DataFrame
    âœ… ä½¿ç”¨ overlay æ¨¡å¼ï¼Œé¿å…é‡å¤å†™å…¥æ—¶æŠ¥é”™
    """

    import pandas as pd

    with pd.ExcelWriter(
        output_file,
        engine="openpyxl",
        mode="a",
        if_sheet_exists="overlay"   # âœ… å…è®¸å¤šæ¬¡å†™åŒä¸€ Sheet
    ) as writer:

        start_row = 0

        def write_block(title, df_block, start_row):
            """
            âœ… å®‰å…¨å†™å…¥å•ä¸ªåŒºå—ï¼š
            - df_block ä¸º None æˆ–ç©ºè¡¨ â†’ è‡ªåŠ¨è·³è¿‡
            - è¿”å›æ–°çš„ start_row
            """

            if df_block is None:
                print(f"âš ï¸ è·³è¿‡åŒºå—ï¼ˆNoneï¼‰ï¼š{title}")
                return start_row

            if isinstance(df_block, pd.DataFrame) and df_block.empty:
                print(f"âš ï¸ è·³è¿‡åŒºå—ï¼ˆç©ºè¡¨ï¼‰ï¼š{title}")
                return start_row

            # ===== æ ‡é¢˜ï¼ˆå•ç‹¬ä¸€è¡Œï¼‰=====
            title_df = pd.DataFrame([[title]])
            title_df.to_excel(
                writer,
                sheet_name=sheet_name,
                startrow=start_row,
                startcol=0,
                index=False,
                header=False
            )

            # ===== æ•°æ®è¡¨ =====
            df_block.to_excel(
                writer,
                sheet_name=sheet_name,
                startrow=start_row + 2,  # æ ‡é¢˜ä¸‹é¢ç©ºä¸€è¡Œ
                startcol=0,
                index=False
            )

            # âœ… è¿”å›ä¸‹ä¸€ä¸ª block çš„èµ·å§‹è¡Œ
            return start_row + len(df_block) + 5

        # ===== âœ… ä¾æ¬¡å†™å…¥å„ä¸ªç»Ÿè®¡å—ï¼ˆå…¨éƒ¨æ˜¯å®‰å…¨å†™å…¥ï¼‰=====
        start_row = write_block("ã€ç»Ÿè®¡ä¸€ï¼šè¯å“ç±»åˆ«ä¸€ã€‘", stat_cat1, start_row)
        start_row = write_block("ã€ç»Ÿè®¡äºŒï¼šç²—åˆ†ç±»ã€‘", stat_coarse, start_row)
        start_row = write_block("ã€ç»Ÿè®¡ä¸‰ï¼šç»†åˆ†ç±»ã€‘", stat_fine, start_row)
        start_row = write_block("ã€ç»Ÿè®¡å››ï¼šç–¾ç—…é¢†åŸŸã€‘", stat_disease_area, start_row)
        start_row = write_block("ã€ç»Ÿè®¡äº”ï¼šé¶ç‚¹ Top10 + Othersã€‘", summary_target, start_row)
        start_row = write_block("ã€ç»Ÿè®¡å…­ï¼šé¶ç‚¹å…¨é‡æ˜ç»†ã€‘", detail_target, start_row)

    print(f"âœ… æ‰€æœ‰å¯ç”¨çš„ Step 3â€“5 ç»Ÿè®¡ç»“æœå·²åˆå¹¶ä¿å­˜åˆ°åŒä¸€ä¸ª Sheetï¼š{sheet_name}")


############### å¤šå­£åº¦åˆå¹¶ ############3

def load_and_merge_by_sheet(
    q_files: list,        # ["Q1.xlsx", "Q2.xlsx", "Q3.xlsx", "Q4.xlsx"]
    sheet_keyword: str    # "FDA" / "NMPA" / "IND" / "NDA"
):
    """
    âœ… ç»ˆæç¨³å¥ç‰ˆï¼ˆå¸¦â€œè¡Œæ•°ç›‘æ§â€+ è‡ªåŠ¨å‰”é™¤ç©ºè¡Œï¼‰ï¼š
    1ï¼‰è‡ªåŠ¨æŸ¥æ‰¾â€œåŒ…å«å…³é”®è¯â€çš„ Sheet
    2ï¼‰å®šä½ã€è¯å“ç±»åˆ«äºŒ / è¯å“ç±»åˆ«ä¸€ã€‘ä½œä¸ºçœŸå®è¡¨å¤´
    3ï¼‰è¯†åˆ« Q1â€“Q4 â†’ å†™å…¥ã€å­£åº¦æ¥æºã€‘
    4ï¼‰æŒ‰ä¸åŒ Sheet ç±»å‹è¿›è¡Œåˆ—è£å‰ª
    5ï¼‰è‡ªåŠ¨å‰”é™¤â€œå…¨ä¸ºç©ºâ€çš„è¡Œ
    6ï¼‰çºµå‘åˆå¹¶
    âœ… å…¨æµç¨‹æ‰“å°â€œæ¯ä¸€æ­¥çš„è¡Œæ•°â€
    """

    dfs = []

    for f in q_files:
        # ===== âœ… 1ï¸âƒ£ è‡ªåŠ¨æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„ sheet =====
        xl = pd.ExcelFile(f, engine="openpyxl")
        matched_sheets = [
            s for s in xl.sheet_names
            if sheet_keyword.lower() in s.lower()
        ]

        if len(matched_sheets) == 0:
            print(f"âš ï¸ æ–‡ä»¶ {f} ä¸­æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€{sheet_keyword}ã€‘çš„ Sheetï¼Œå·²è·³è¿‡")
            continue

        sheet_name = matched_sheets[0]
        print(f"\nâœ… æ–‡ä»¶ {f} ä½¿ç”¨ Sheet: {sheet_name}")

        # ===== âœ… 2ï¸âƒ£ æ— è¡¨å¤´è¯»å–ï¼Œç”¨äºâ€œå®šä½çœŸå®è¡¨å¤´è¡Œâ€ =====
        df_raw = pd.read_excel(
            f,
            sheet_name=sheet_name,
            engine="openpyxl",
            header=None
        )

        # åœ¨å‰ 10 è¡Œå†…æŸ¥æ‰¾â€œè¯å“ç±»åˆ«äºŒâ€æˆ–â€œè¯å“ç±»åˆ«ä¸€â€ä½œä¸ºè¡¨å¤´å®šä½é”šç‚¹
        header_row_idx = None
        search_limit = min(10, len(df_raw))

        for i in range(search_limit):
            row_vals = df_raw.iloc[i].astype(str).tolist()
            if any(v.strip() in ["è¯å“ç±»åˆ«äºŒ", "è¯å“ç±»åˆ«ä¸€"] for v in row_vals):
                header_row_idx = i
                break

        if header_row_idx is not None:
            if header_row_idx > 0:
                print(f"    âœ… åœ¨ç¬¬ {header_row_idx+1} è¡Œæ£€æµ‹åˆ°çœŸå®è¡¨å¤´ï¼Œå·²è‡ªåŠ¨åˆ é™¤å‰ {header_row_idx} è¡Œ")

            new_header = df_raw.iloc[header_row_idx].astype(str).values
            df = df_raw.iloc[header_row_idx + 1:].copy()
            df.columns = new_header
            df = df.reset_index(drop=True)
        else:
            print(f"    âš ï¸ æœªåœ¨å‰ 10 è¡Œä¸­æ£€æµ‹åˆ°ã€è¯å“ç±»åˆ«ä¸€/äºŒã€‘ï¼Œé€€å›é»˜è®¤è¯»å–æ–¹å¼")
            df = pd.read_excel(f, sheet_name=sheet_name, engine="openpyxl")

        # âœ…âœ…âœ… ç¬¬ä¸€æ¬¡å‰”é™¤â€œå…¨ä¸ºç©ºâ€çš„è¡Œï¼ˆè¡¨å¤´ä¿®å¤åï¼‰
        before_drop = df.shape[0]
        df = df.dropna(how="all").reset_index(drop=True)
        after_drop = df.shape[0]
        print(f"    ğŸ§¹ è¡¨å¤´ä¿®å¤åï¼šå‰”é™¤ç©ºè¡Œ {before_drop - after_drop} è¡Œ")

        # âœ… æ‰“å°ï¼šä¿®å¤è¡¨å¤´åçš„â€œæœ‰æ•ˆè¡Œæ•°â€
        print(f"    ğŸ“Œ è¡¨å¤´ä¿®å¤åæœ‰æ•ˆè¡Œæ•°ï¼š{df.shape[0]}")

        # ===== âœ… 3ï¸âƒ£ æ ‡è®°å­£åº¦æ¥æº Q1-Q4 =====
        fname = os.path.basename(f).upper()

        if "Q1" in fname:
            quarter = "Q1"
        elif "Q2" in fname:
            quarter = "Q2"
        elif "Q3" in fname:
            quarter = "Q3"
        elif "Q4" in fname:
            quarter = "Q4"
        else:
            quarter = "æœªçŸ¥å­£åº¦"
            print(f"âš ï¸ æ— æ³•ä»æ–‡ä»¶åè¯†åˆ«å­£åº¦ï¼š{f}")

        df["å­£åº¦æ¥æº"] = quarter

        # âœ… æ‰“å°ï¼šåŠ å­£åº¦åçš„â€œæœ‰æ•ˆè¡Œæ•°â€
        print(f"    ğŸ“Œ æ·»åŠ å­£åº¦æ¥æºåè¡Œæ•°ï¼š{df.shape[0]}")

        # ===== âœ… 4ï¸âƒ£ æŒ‰ Sheet ç±»å‹è£å‰ªåˆ—ï¼ˆæœ€ç»ˆå£å¾„ï¼‰ =====
        keyword_upper = sheet_keyword.upper()

        if keyword_upper == "FDA":
            base_keep_cols = ["é€šç”¨å", "å‰‚å‹", "é›†å›¢", "è¯å“ç±»åˆ«ä¸€", "è¯å“ç±»åˆ«äºŒ", "é¶ç‚¹", "å­£åº¦æ¥æº"]

        elif keyword_upper == "NMPA":
            base_keep_cols = ["é€šç”¨å", "è¯å“ç±»åˆ«ä¸€", "é¶ç‚¹", "å­£åº¦æ¥æº"]

        elif keyword_upper in ["IND", "NDA"]:
            base_keep_cols = ["é€šç”¨å", "è¯å“ç±»åˆ«ä¸€", "è¯å“ç±»åˆ«äºŒ", "é¶ç‚¹", "å‚è€ƒç–¾ç—…é¢†åŸŸ", "å­£åº¦æ¥æº"]

        else:
            base_keep_cols = []

        if base_keep_cols:
            keep_cols = [c for c in base_keep_cols if c in df.columns]
            missing = set(base_keep_cols) - set(keep_cols)
            if missing:
                print(f"âš ï¸ {sheet_keyword} ä¸­ç¼ºå¤±éƒ¨åˆ†æœŸæœ›åˆ—ï¼š{missing}")

            df = df[keep_cols].copy()

        # âœ…âœ…âœ… ç¬¬äºŒæ¬¡å‰”é™¤â€œè£å‰ªåå¯èƒ½å½¢æˆçš„ç©ºè¡Œâ€
        before_drop2 = df.shape[0]
        df = df.dropna(how="all").reset_index(drop=True)
        after_drop2 = df.shape[0]
        print(f"    ğŸ§¹ è£å‰ªåï¼šå‰”é™¤ç©ºè¡Œ {before_drop2 - after_drop2} è¡Œ")

        # âœ… æ‰“å°ï¼šè£å‰ªåçš„â€œæœ€ç»ˆæœ‰æ•ˆè¡Œæ•°â€
        print(f"    âœ… è£å‰ªåæœ€ç»ˆæœ‰æ•ˆè¡Œæ•°ï¼š{df.shape[0]}")

        dfs.append(df)

    if len(dfs) == 0:
        raise ValueError(f"âŒ æ‰€æœ‰æ–‡ä»¶ä¸­å‡æœªæˆåŠŸè¯»å–åˆ°ã€{sheet_keyword}ã€‘ç›¸å…³ Sheet")

    df_all = pd.concat(dfs, ignore_index=True)

    print(f"\nâœ… å·²åˆå¹¶ Sheet å…³é”®è¯ = {sheet_keyword}")
    print(f"âœ… åˆå¹¶åæ€»æœ‰æ•ˆè¡Œæ•°ï¼š{df_all.shape[0]}")

    return df_all


#################### å•ä¸ªå­£åº¦æ•°æ®å¤„ç†å°è£… ####################

#  NMPA
def run_nmpa_quarter_pipeline(
    input_file: str,
    output_file: str,
    year: int,
    quarter: str,
    sheet_name: str = "æ•°æ®è¯¦æƒ…",
    approval_date_col: str = "æœ€æ–°æ‰¹å‡†æ—¥æœŸ",
    drug_name_col: str = "é€šç”¨å",
    disease_col: str = "å‚è€ƒç–¾ç—…é¢†åŸŸ",
    target_col: str = "é¶ç‚¹",
    summary_sheet_name: str = "æ‰€æœ‰ç»Ÿè®¡æ±‡æ€»"
):
    """
    âœ… NMPA æœ€è¿‘ä¸€å­£åº¦â€œå…¨è‡ªåŠ¨ç»Ÿè®¡æµæ°´çº¿â€ï¼š
    1ï¸âƒ£ æœ€è¿‘ä¸€å­£åº¦æ‰¹å‡† + åŒè¯åŒåºå·
    2ï¸âƒ£ åˆ†ç±»æ˜ å°„
    3ï¸âƒ£ ä¿å­˜åˆ†ç±»æ˜ç»†è¡¨
    4ï¸âƒ£ è¯å“ç±»åˆ«ä¸€ / ç²—åˆ† / ç»†åˆ† ç»Ÿè®¡
    5ï¸âƒ£ å‚è€ƒç–¾ç—…é¢†åŸŸç»Ÿè®¡
    6ï¸âƒ£ é¶ç‚¹ Top10 + Others ç»Ÿè®¡
    7ï¸âƒ£ æ‰€æœ‰ç»Ÿè®¡ç»“æœå†™å…¥åŒä¸€ Sheet

    âœ… ä½ åªéœ€è¦ä¼ ï¼šinput_file, output_file, year, quarter
    """

    print("\n===============================")
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œ NMPA {year} {quarter} ç»Ÿè®¡æµæ°´çº¿")
    print("===============================\n")

    # ===== 1ï¸âƒ£ æœ€è¿‘ä¸€å­£åº¦æ‰¹å‡† + åŒè¯åŒåºå· =====
    df_dedup = step1_nmpa_filter_by_quarter(
        input_path=input_file,
        sheet_name=sheet_name,
        approval_date_col=approval_date_col,
        drug_name_col=drug_name_col,
        year=year,
        quarter=quarter
    )

    # ===== 2ï¸âƒ£ æ„å»ºåˆ†ç±»è§„åˆ™ =====
    df_map = build_classify_mapping_from_json()

    # ===== 3ï¸âƒ£ åŠ åˆ†ç±» & âœ… ä¿å­˜åˆ†ç±»æ˜ç»†è¡¨ =====
    df_with_class = step2_add_class_and_save(
        df=df_dedup,
        df_map=df_map,
        output_classified_path=output_file
    )

    # ===== 4ï¸âƒ£ âœ… åˆ†ç±»ç»Ÿè®¡ï¼ˆè¯å“ç±»åˆ«ä¸€ / ç²—åˆ† / ç»†åˆ†ï¼‰=====
    stat_cat1, stat_coarse, stat_fine = step3_print_statistics(df_with_class)

    # ===== 5ï¸âƒ£ âœ… ç–¾ç—…é¢†åŸŸç»Ÿè®¡ =====
    stat_disease_area = step4_statistics_by_disease_area(
        df_with_class,
        disease_col=disease_col
    )

    # ===== 6ï¸âƒ£ âœ… é¶ç‚¹ Top10 + Others =====
    detail_target, summary_target = step5_statistics_by_target(
        df_with_class,
        target_col=target_col
    )

    # ===== 7ï¸âƒ£ âœ… æ‰€æœ‰ç»Ÿè®¡ç»“æœåˆå¹¶å†™å…¥åŒä¸€ä¸ª Sheet =====
    save_all_stats_to_one_sheet(
        output_file=output_file,
        stat_cat1=stat_cat1,
        stat_coarse=stat_coarse,
        stat_fine=stat_fine,
        stat_disease_area=stat_disease_area,
        summary_target=summary_target,
        detail_target=detail_target,
        sheet_name=summary_sheet_name
    )

    print("\n===============================")
    print("âœ… NMPA æœ€è¿‘ä¸€å­£åº¦ç»Ÿè®¡æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶ï¼š{output_file}")
    print("===============================\n")

    return {
    "df": df_with_class,
    "stat_cat1": stat_cat1,
    "stat_coarse":stat_coarse,
    "stat_fine":stat_fine,
    "stat_disease": stat_disease_area,
    "stat_target": summary_target
}


######## FDA
def run_fda_pipeline(
    input_file: str,
    output_file: str,
    sheet_name: str = "ç›®æ ‡è¯å“",
    target_col: str = "é¶ç‚¹",
    summary_sheet_name: str = "æ‰€æœ‰ç»Ÿè®¡æ±‡æ€»"
):
    """
    âœ… FDA å…¨è‡ªåŠ¨ç»Ÿè®¡æµæ°´çº¿ï¼š
    1ï¸âƒ£ ç›®æ ‡è¯å“å»é‡ + åŠ åºå·
    2ï¸âƒ£ åˆ†ç±»æ˜ å°„
    3ï¸âƒ£ ä¿å­˜åˆ†ç±»æ˜ç»†è¡¨
    4ï¸âƒ£ è¯å“ç±»åˆ«ä¸€ / ç²—åˆ† / ç»†åˆ† ç»Ÿè®¡
    5ï¸âƒ£ é¶ç‚¹ Top10 + Others ç»Ÿè®¡
    6ï¸âƒ£ æ‰€æœ‰ç»Ÿè®¡ç»“æœå†™å…¥åŒä¸€ Sheet

    âœ… FDA ä¸åšç–¾ç—…é¢†åŸŸç»Ÿè®¡ï¼ˆè‡ªåŠ¨ä¼  Noneï¼‰
    """

    print("\n===============================")
    print("ğŸš€ å¼€å§‹æ‰§è¡Œ FDA ç»Ÿè®¡æµæ°´çº¿")
    print("===============================\n")

    # ===== 1ï¸âƒ£ FDAï¼šç›®æ ‡è¯å“å»é‡ + åŠ åºå· =====
    df_dedup = step1_fda_dedup_and_add_id(
        input_path=input_file,
        sheet_name=sheet_name
    )

    # ===== 2ï¸âƒ£ æ„å»ºåˆ†ç±»è§„åˆ™ =====
    df_map = build_classify_mapping_from_json()

    # ===== 3ï¸âƒ£ åŠ åˆ†ç±» & âœ… ä¿å­˜åˆ†ç±»æ˜ç»†è¡¨ =====
    df_with_class = step2_add_class_and_save(
        df=df_dedup,
        df_map=df_map,
        output_classified_path=output_file
    )

    # ===== 4ï¸âƒ£ âœ… åˆ†ç±»ç»Ÿè®¡ï¼ˆè¯å“ç±»åˆ«ä¸€ / ç²—åˆ† / ç»†åˆ†ï¼‰=====
    stat_cat1, stat_coarse, stat_fine = step3_print_statistics(df_with_class)

    # ===== 5ï¸âƒ£ âœ… é¶ç‚¹ Top10 + Others =====
    detail_target, summary_target = step5_statistics_by_target(
        df_with_class,
        target_col=target_col
    )

    # ===== 6ï¸âƒ£ âœ… æ‰€æœ‰ç»Ÿè®¡ç»“æœåˆå¹¶å†™å…¥åŒä¸€ä¸ª Sheet =====
    save_all_stats_to_one_sheet(
        output_file=output_file,
        stat_cat1=stat_cat1,
        stat_coarse=stat_coarse,
        stat_fine=stat_fine,
        stat_disease_area=None,      # âœ… FDA æ— ç–¾ç—…é¢†åŸŸ
        summary_target=summary_target,
        detail_target=detail_target,
        sheet_name=summary_sheet_name
    )

    print("\n===============================")
    print("âœ… FDA ç»Ÿè®¡æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶ï¼š{output_file}")
    print("===============================\n")

    return {
    "df": df_with_class,
    "stat_cat1": stat_cat1,
    "stat_coarse":stat_coarse,
    "stat_fine":stat_fine,
    "stat_disease": None,
    "stat_target": summary_target
}

def run_ind_nda_pipeline(
    input_file: str,
    output_file: str,
    source: str,   # "IND" æˆ– "NDA"
    disease_col: str = "å‚è€ƒç–¾ç—…é¢†åŸŸ",
    target_col: str = "é¶ç‚¹",
    summary_sheet_name: str = "æ‰€æœ‰ç»Ÿè®¡æ±‡æ€»"
):
    """
    âœ… IND / NDA é€šç”¨å…¨è‡ªåŠ¨ç»Ÿè®¡æµæ°´çº¿ï¼š
    1ï¸âƒ£ å»é‡ï¼ˆé€šç”¨å + å‰‚å‹ + æŒè¯å•†ï¼Œä¿ç•™æœ€æ–°ï¼‰
    2ï¸âƒ£ åˆ†ç±»æ˜ å°„
    3ï¸âƒ£ ä¿å­˜åˆ†ç±»æ˜ç»†è¡¨
    4ï¸âƒ£ è¯å“ç±»åˆ«ä¸€ / ç²—åˆ† / ç»†åˆ† ç»Ÿè®¡
    5ï¸âƒ£ å‚è€ƒç–¾ç—…é¢†åŸŸç»Ÿè®¡
    6ï¸âƒ£ é¶ç‚¹ Top10 + Others
    7ï¸âƒ£ æ‰€æœ‰ç»Ÿè®¡ç»“æœå†™å…¥åŒä¸€ Sheet

    âœ… source åªèƒ½æ˜¯ï¼š"IND" æˆ– "NDA"
    """

    source = source.upper()
    if source not in ["IND", "NDA"]:
        raise ValueError("âŒ source åªèƒ½æ˜¯ 'IND' æˆ– 'NDA'")

    print("\n===============================")
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œ {source} ç»Ÿè®¡æµæ°´çº¿")
    print("===============================\n")

    if isinstance(input_file, str):
        # ===== 1ï¸âƒ£ å»é‡ï¼ˆä»…å†…å­˜ä¸­ï¼‰ =====
        df_dedup = step1_dedup_only_keep_latest_NDA_IND(
            input_path=input_file
        )
    else:
        df_dedup=input_file

    # ===== 2ï¸âƒ£ æ„å»ºåˆ†ç±»è§„åˆ™ =====
    df_map = build_classify_mapping_from_json()

    # ===== 3ï¸âƒ£ åŠ åˆ†ç±» & âœ… ä¿å­˜åˆ†ç±»æ˜ç»†è¡¨ =====
    df_with_class = step2_add_class_and_save(
        df=df_dedup,
        df_map=df_map,
        output_classified_path=output_file
    )

    # ===== 4ï¸âƒ£ âœ… åˆ†ç±»ç»Ÿè®¡ï¼ˆè¯å“ç±»åˆ«ä¸€ / ç²—åˆ† / ç»†åˆ†ï¼‰=====
    stat_cat1, stat_coarse, stat_fine = step3_print_statistics(df_with_class)

    # ===== 5ï¸âƒ£ âœ… ç–¾ç—…é¢†åŸŸç»Ÿè®¡ =====
    stat_disease_area = step4_statistics_by_disease_area(
        df_with_class,
        disease_col=disease_col
    )

    # ===== 6ï¸âƒ£ âœ… é¶ç‚¹ Top10 + Others =====
    detail_target, summary_target = step5_statistics_by_target(
        df_with_class,
        target_col=target_col
    )

    # ===== 7ï¸âƒ£ âœ… æ‰€æœ‰ç»Ÿè®¡ç»“æœåˆå¹¶å†™å…¥åŒä¸€ä¸ª Sheet =====
    save_all_stats_to_one_sheet(
        output_file=output_file,
        stat_cat1=stat_cat1,
        stat_coarse=stat_coarse,
        stat_fine=stat_fine,
        stat_disease_area=stat_disease_area,
        summary_target=summary_target,
        detail_target=detail_target,
        sheet_name=summary_sheet_name
    )

    print("\n===============================")
    print(f"âœ… {source} ç»Ÿè®¡æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶ï¼š{output_file}")
    print("===============================\n")

    return {
    "df": df_with_class,
    "stat_cat1": stat_cat1,
    "stat_coarse":stat_coarse,
    "stat_fine":stat_fine,
    "stat_disease": stat_disease_area,
    "stat_target": summary_target
}



def align_and_export_to_self_template_by_json(
    template_json_path: str,         # âœ… ä½ ä¿å­˜çš„ template_columns.json
    output_excel_path: str,          # æ–°å¯¼å‡ºçš„ç»“æœ Excel
    df_nmpa: pd.DataFrame,
    df_fda: pd.DataFrame,
    df_ind: pd.DataFrame,
    df_nda: pd.DataFrame,
    stats_dict: dict                 # æ¯ç±»å¯¹åº”çš„ç»Ÿè®¡ç»“æœ
):
    """
    âœ… åŠŸèƒ½ï¼ˆJSON é©±åŠ¨æœ€ç»ˆç‰ˆï¼‰ï¼š
    1ï¸âƒ£ ä» JSON è¯»å– 4 ä¸ª Sheet çš„ã€æ ‡å‡†åˆ—ç»“æ„ã€‘
    2ï¸âƒ£ è‡ªåŠ¨å¯¹é½æ–°æ•°æ®åˆ—å
    3ï¸âƒ£ å†™å›ä¸º 4 ä¸ªå­ Sheet
    4ï¸âƒ£ åœ¨æ¯ä¸ª Sheet ä¸‹æ–¹è¿½åŠ å¯¹åº”ç»Ÿè®¡è¡¨
    âœ… ç‰¹åˆ«è§„åˆ™ï¼š
       - å¦‚æœæ¨¡æ¿ä¸­æœ‰åˆ—ã€ç±»å‹ã€‘ï¼Œä¸”ä¸­é—´ df ä¸­æœ‰ã€ç±»åˆ«(ç²—åˆ†)ã€‘åˆ—ï¼Œ
         åˆ™è‡ªåŠ¨ç”¨ã€ç±»åˆ«(ç²—åˆ†)ã€‘å¡«å……ã€ç±»å‹ã€‘
    """

    # ===== âœ… 0ï¸âƒ£ è¯»å– JSON æ¨¡æ¿åˆ—é…ç½® =====
    with open(template_json_path, "r", encoding="utf-8") as f:
        template_cols_map = json.load(f)

    sheet_map = {
        "NMPA approved drugs": df_nmpa,
        "FDA approved drugs": df_fda,
        "China IND": df_ind,
        "China NDA": df_nda,
    }

    # âœ… ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    save_dir = os.path.dirname(output_excel_path)
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)

    with pd.ExcelWriter(output_excel_path, engine="openpyxl") as writer:
        for sheet_name, df_new in sheet_map.items():

            print(f"\nâœ… æ­£åœ¨å¤„ç† Sheetï¼š{sheet_name}")

            # ===== âœ… 1ï¸âƒ£ ä» JSON è¯»å–æ ‡å‡†åˆ—ç»“æ„ =====
            template_cols = template_cols_map.get(sheet_name)

            if not template_cols:
                print(f"âš ï¸ JSON ä¸­æœªæ‰¾åˆ°è¯¥ Sheet çš„åˆ—æ¨¡æ¿ï¼š{sheet_name}ï¼Œå·²è·³è¿‡")
                continue

            print("    ğŸ”¹ æ¨¡æ¿åˆ—åï¼ˆæ¥è‡ª JSONï¼‰ï¼š", template_cols)

            # ===== âœ… 2ï¸âƒ£ æŒ‰æ¨¡æ¿åˆ—æ„é€ å¯¹é½åçš„ DataFrame =====
            aligned_data = {}
            n_rows = len(df_new)

            for col in template_cols:
                if col in df_new.columns:
                    # æ¨¡æ¿åˆ—ååœ¨æ–°æ•°æ®ä¸­ä¹Ÿå­˜åœ¨ â†’ ç›´æ¥ç”¨
                    aligned_data[col] = df_new[col].values

                elif col == "ç±»å‹" and "ç±»åˆ«(ç²—åˆ†)" in df_new.columns:
                    # âœ… ç‰¹æ®Šè§„åˆ™ï¼šæ¨¡æ¿éœ€è¦ã€ç±»å‹ã€‘ï¼Œç”¨ä¸­é—´ df çš„ã€ç±»åˆ«(ç²—åˆ†)ã€‘æ¥å¡«
                    print("    ğŸ” åˆ—ã€ç±»å‹ã€‘ä½¿ç”¨ä¸­é—´æ•°æ®åˆ—ã€ç±»åˆ«(ç²—åˆ†)ã€‘è¿›è¡Œå¡«å……")
                    aligned_data[col] = df_new["ç±»åˆ«(ç²—åˆ†)"].values

                else:
                    # æ¨¡æ¿æœ‰ï¼Œä½†æ–° df æ²¡æœ‰ï¼Œè¡¥ç©º
                    aligned_data[col] = [pd.NA] * n_rows

            df_aligned = pd.DataFrame(aligned_data, columns=template_cols)

            print(f"    âœ… åˆ—å¯¹é½å®Œæˆï¼Œæœ€ç»ˆåˆ—æ•°ï¼š{len(df_aligned.columns)}")
            print(f"    âœ… æ•°æ®è¡Œæ•°ï¼š{len(df_aligned)}")

            # ===== âœ… 3ï¸âƒ£ å†™å…¥ä¸»æ•°æ® =====
            df_aligned.to_excel(
                writer,
                sheet_name=sheet_name,
                index=False,
                startrow=0
            )

            start_row = len(df_aligned) + 3  # ç©ºä¸¤è¡Œå†å†™ç»Ÿè®¡

            # ===== âœ… 4ï¸âƒ£ è¿½åŠ ç»Ÿè®¡è¡¨ =====
            stat_pack = stats_dict.get(sheet_name, {})

            for title, stat_df in stat_pack.items():
                if stat_df is None or stat_df.empty:
                    continue

                title_df = pd.DataFrame([[title]])
                title_df.to_excel(
                    writer,
                    sheet_name=sheet_name,
                    startrow=start_row,
                    index=False,
                    header=False
                )

                stat_df.to_excel(
                    writer,
                    sheet_name=sheet_name,
                    startrow=start_row + 2,
                    index=False
                )

                start_row += len(stat_df) + 4

            print(f"    âœ… {sheet_name} å†™å…¥å®Œæˆ")

    print("\n===============================")
    print("âœ… å·²å®Œå…¨æŒ‰ã€JSON æ¨¡æ¿ç»“æ„ã€‘å¯¼å‡ºæ–°ç‰ˆæœ¬")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{output_excel_path}")
    print(f"ğŸ“Œ æ¨¡æ¿æ¥æºï¼š{template_json_path}")
    print("===============================\n")